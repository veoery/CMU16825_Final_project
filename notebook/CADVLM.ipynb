{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLhILwWZkf0q"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import json\n",
        "# from tqdm import tqdm\n",
        "# import random\n",
        "\n",
        "# # ------------------------------------------------------------------\n",
        "# # Roots\n",
        "# # ------------------------------------------------------------------\n",
        "# geom_root = \"/content/drive/MyDrive/1_Diamond/CADVLM/Omni-CAD-subset-complete/json\"\n",
        "# txt_root  = \"/content/drive/MyDrive/1_Diamond/CADVLM/Omni-CAD-subset-complete/txt\"\n",
        "# img_root  = \"/content/drive/MyDrive/1_Diamond/CADVLM/Omni-CAD-subset-complete/img\"\n",
        "# pts_root  = \"/content/drive/MyDrive/1_Diamond/CADVLM/Omni-CAD-subset-complete/pointcloud\"\n",
        "\n",
        "# # how many rendered views per shape? adjust if your dataset uses a different number\n",
        "# NUM_VIEWS = 8\n",
        "\n",
        "# # ------------------------------------------------------------------\n",
        "# # Deterministic path builders (no directory listing needed)\n",
        "# # ------------------------------------------------------------------\n",
        "# def get_image_paths_for_shape(img_root, category, shape_id, num_views=NUM_VIEWS):\n",
        "#     \"\"\"\n",
        "#     Build image paths using the known naming convention:\n",
        "#     img_root/category/shape_id_000.png ... _00{num_views-1}.png\n",
        "\n",
        "#     Only returns paths that actually exist.\n",
        "#     \"\"\"\n",
        "#     cat_dir = os.path.join(img_root, category)\n",
        "#     if not os.path.isdir(cat_dir):\n",
        "#         return []\n",
        "\n",
        "#     paths = []\n",
        "#     for i in range(num_views):\n",
        "#         fname = f\"{shape_id}_{i:03d}.png\"\n",
        "#         path = os.path.join(cat_dir, fname)\n",
        "#         if os.path.exists(path):\n",
        "#             paths.append(path)\n",
        "\n",
        "#     return paths\n",
        "\n",
        "\n",
        "# def get_pointcloud_paths_for_shape(pts_root, category, shape_id):\n",
        "#     \"\"\"\n",
        "#     Build pointcloud path using known naming convention:\n",
        "#     pointcloud/category/shape_id.ext\n",
        "\n",
        "#     We try a few typical extensions in a fixed order.\n",
        "#     \"\"\"\n",
        "#     cat_dir = os.path.join(pts_root, category)\n",
        "#     if not os.path.isdir(cat_dir):\n",
        "#         return []\n",
        "\n",
        "#     exts = [\".npz\", \".npy\", \".ply\", \".pcd\"]\n",
        "#     paths = []\n",
        "#     for ext in exts:\n",
        "#         candidate = os.path.join(cat_dir, shape_id + ext)\n",
        "#         if os.path.exists(candidate):\n",
        "#             paths.append(candidate)\n",
        "#             # usually there's just one, but keeping a list for consistency\n",
        "#     return paths\n",
        "\n",
        "\n",
        "# # ------------------------------------------------------------------\n",
        "# # Main builder\n",
        "# # ------------------------------------------------------------------\n",
        "# def build_omnicad_multimodal_pairs(geom_root, txt_root, img_root, pts_root,\n",
        "#                                    max_pairs=None):\n",
        "#     \"\"\"\n",
        "#     Build a list of examples where each example is:\n",
        "#       {\n",
        "#         \"id\":               \"0003/00038800_00007\",\n",
        "#         \"category\":         \"0003\",\n",
        "#         \"shape_id\":         \"00038800_00007\",\n",
        "#         \"description\":      <natural language text>,\n",
        "#         \"json_text\":        <raw Omni-CAD JSON string>,\n",
        "#         \"image_paths\":      [list of image file paths],\n",
        "#         \"pointcloud_paths\": [list of point cloud file paths],\n",
        "#       }\n",
        "\n",
        "#     Here we assume strict naming conventions and avoid any directory scanning\n",
        "#     for images / pointclouds.\n",
        "#     \"\"\"\n",
        "\n",
        "#     # ---- 1. Build caption index: id -> description ----\n",
        "#     caption_by_id = {}\n",
        "\n",
        "#     txt_files = [f for f in os.listdir(txt_root) if f.endswith(\".json\")]\n",
        "#     txt_files = sorted(txt_files)\n",
        "\n",
        "#     print(f\"Found {len(txt_files)} caption files under txt_root\")\n",
        "\n",
        "#     for fname in tqdm(txt_files, desc=\"Reading caption files\"):\n",
        "#         path = os.path.join(txt_root, fname)\n",
        "#         with open(path, \"r\") as f:\n",
        "#             arr = json.load(f)\n",
        "\n",
        "#         # each element like: {\"id\": \"0003/00038800_00007\", \"text caption\": \"...\"}\n",
        "#         for entry in arr:\n",
        "#             sid = entry.get(\"id\")\n",
        "#             if sid is None:\n",
        "#                 continue\n",
        "\n",
        "#             caption = (\n",
        "#                 entry.get(\"text caption\")\n",
        "#                 or entry.get(\"caption\")\n",
        "#                 or entry.get(\"text\")\n",
        "#             )\n",
        "#             if caption is None:\n",
        "#                 continue\n",
        "\n",
        "#             caption_by_id[sid] = caption\n",
        "\n",
        "#     print(f\"Collected {len(caption_by_id)} unique ids with captions\")\n",
        "\n",
        "#     # ---- 2. For each id, construct paths directly from naming scheme ----\n",
        "#     pairs = []\n",
        "#     for sid, caption in tqdm(\n",
        "#         caption_by_id.items(),\n",
        "#         total=len(caption_by_id),\n",
        "#         desc=\"Building multimodal pairs\"\n",
        "#     ):\n",
        "#         # sid like \"0000/00000071_00005\"\n",
        "#         try:\n",
        "#             cat, shape = sid.split(\"/\")\n",
        "#         except ValueError:\n",
        "#             # unexpected id format, skip\n",
        "#             continue\n",
        "\n",
        "#         # geometry path: json_root/cat/shape.json\n",
        "#         geom_path = os.path.join(geom_root, cat, shape + \".json\")\n",
        "#         if not os.path.exists(geom_path):\n",
        "#             # geometry missing, skip\n",
        "#             continue\n",
        "\n",
        "#         # read raw JSON string (faster than parse+re-dump)\n",
        "#         with open(geom_path, \"r\") as f:\n",
        "#             json_text = f.read()\n",
        "\n",
        "#         # images & pointclouds: constructed from known naming convention\n",
        "#         image_paths = get_image_paths_for_shape(img_root, cat, shape)\n",
        "#         pointcloud_paths = get_pointcloud_paths_for_shape(pts_root, cat, shape)\n",
        "\n",
        "#         pairs.append({\n",
        "#             \"id\": sid,\n",
        "#             \"category\": cat,\n",
        "#             \"shape_id\": shape,\n",
        "#             \"description\": caption,\n",
        "#             \"json_text\": json_text,\n",
        "#             \"image_paths\": image_paths,\n",
        "#             \"pointcloud_paths\": pointcloud_paths,\n",
        "#         })\n",
        "\n",
        "#         if max_pairs is not None and len(pairs) >= max_pairs:\n",
        "#             break\n",
        "\n",
        "#     print(f\"Total multimodal examples: {len(pairs)}\")\n",
        "#     return pairs\n",
        "\n",
        "# # ------------------------------------------------------------------\n",
        "# # Run it\n",
        "# # ------------------------------------------------------------------\n",
        "# pairs = build_omnicad_multimodal_pairs(\n",
        "#     geom_root, txt_root, img_root, pts_root,\n",
        "#     max_pairs=None   # or a small number (e.g. 1000) for a test\n",
        "# )\n",
        "\n",
        "# if pairs:\n",
        "#     ex = random.choice(pairs)\n",
        "#     print(\"\\nExample id:\", ex[\"id\"])\n",
        "#     print(\"Category:\", ex[\"category\"], \"Shape:\", ex[\"shape_id\"])\n",
        "#     print(\"Description:\", ex[\"description\"])\n",
        "#     print(\"JSON snippet:\", ex[\"json_text\"][:200])\n",
        "#     print(\"Images:\", ex[\"image_paths\"][:3])\n",
        "#     print(\"Pointclouds:\", ex[\"pointcloud_paths\"][:3])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# out_path = \"/content/drive/MyDrive/1_Diamond/CADVLM/omnicad_multimodal_pairs.jsonl\"\n",
        "# with open(out_path, \"w\") as f:\n",
        "#   for ex in pairs:\n",
        "#       f.write(json.dumps(ex) + \"\\n\")\n",
        "# print(\"Saved pairs to:\", out_path)"
      ],
      "metadata": {
        "id": "ZDWz2VBwOKI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "# from torch.utils.data import Dataset\n",
        "# from PIL import Image\n",
        "# import numpy as np\n",
        "\n",
        "# class OmniCADPairsDataset(Dataset):\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         jsonl_path,\n",
        "#         load_images=False,\n",
        "#         image_transform=None,\n",
        "#         load_pointclouds=False,\n",
        "#     ):\n",
        "#         \"\"\"\n",
        "#         jsonl_path: path to omnicad_multimodal_pairs.jsonl\n",
        "#         load_images: whether to load the first image as a PIL / transformed tensor\n",
        "#         image_transform: torchvision-style transform for images\n",
        "#         load_pointclouds: whether to load the first point cloud (npz/npy)\n",
        "#         \"\"\"\n",
        "#         self.jsonl_path = jsonl_path\n",
        "#         self.load_images = load_images\n",
        "#         self.image_transform = image_transform\n",
        "#         self.load_pointclouds = load_pointclouds\n",
        "\n",
        "#         self.examples = []\n",
        "#         with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "#             for line in f:\n",
        "#                 line = line.strip()\n",
        "#                 if not line:\n",
        "#                     continue\n",
        "#                 self.examples.append(json.loads(line))\n",
        "\n",
        "#         print(f\"Loaded {len(self.examples)} examples from {jsonl_path}\")\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.examples)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         ex = self.examples[idx]\n",
        "\n",
        "#         # core info: always there\n",
        "#         item = {\n",
        "#             \"id\": ex[\"id\"],\n",
        "#             \"category\": ex[\"category\"],\n",
        "#             \"shape_id\": ex[\"shape_id\"],\n",
        "#             \"description\": ex[\"description\"],     # NL text\n",
        "#             \"json_text\": ex[\"json_text\"],         # Omni-CAD JSON as string\n",
        "#             \"image_paths\": ex.get(\"image_paths\", []),\n",
        "#             \"pointcloud_paths\": ex.get(\"pointcloud_paths\", []),\n",
        "#         }\n",
        "\n",
        "#         # --- optionally load first image ---\n",
        "#         if self.load_images and item[\"image_paths\"]:\n",
        "#             img_path = item[\"image_paths\"][0]\n",
        "#             img = Image.open(img_path).convert(\"RGB\")\n",
        "#             if self.image_transform is not None:\n",
        "#                 img = self.image_transform(img)\n",
        "#             item[\"image\"] = img  # only add key if we actually have an image\n",
        "\n",
        "#         # --- optionally load first point cloud ---\n",
        "#         if self.load_pointclouds and item[\"pointcloud_paths\"]:\n",
        "#             pc_path = item[\"pointcloud_paths\"][0]\n",
        "#             pts = None\n",
        "#             if pc_path.endswith(\".npz\"):\n",
        "#                 npz = np.load(pc_path)\n",
        "#                 if \"points\" in npz:\n",
        "#                     pts = npz[\"points\"]\n",
        "#                 else:\n",
        "#                     pts = list(npz.values())[0]\n",
        "#             elif pc_path.endswith(\".npy\"):\n",
        "#                 pts = np.load(pc_path)\n",
        "#             # you can extend this for .ply/.pcd\n",
        "#             item[\"points\"] = pts  # only add if actually loaded\n",
        "\n",
        "#         return item\n"
      ],
      "metadata": {
        "id": "gmLYEEaOPCvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# jsonl_path = \"/content/drive/MyDrive/1_Diamond/CADVLM/omnicad_multimodal_pairs.jsonl\"\n",
        "\n",
        "# dataset = OmniCADPairsDataset(\n",
        "#     jsonl_path,\n",
        "#     load_images=False,\n",
        "#     image_transform=None,\n",
        "#     load_pointclouds=False,\n",
        "# )\n",
        "\n",
        "# loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# batch = next(iter(loader))\n",
        "# print(batch[\"description\"][0])\n",
        "# print(batch[\"json_text\"][0][:200])\n",
        "# print(batch[\"image_paths\"][0])\n",
        "# print(batch[\"pointcloud_paths\"][0])\n"
      ],
      "metadata": {
        "id": "6Hwdn5taPD4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "\n",
        "# save_pt_path = \"/content/drive/MyDrive/1_Diamond/CADVLM/omnicad_multimodal_pairs.pt\"\n",
        "# torch.save(dataset.examples, save_pt_path)\n",
        "# print(\"Saved examples to:\", save_pt_path)\n"
      ],
      "metadata": {
        "id": "xZPherXgQj3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import json\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# from PIL import Image\n",
        "# import matplotlib.pyplot as plt\n",
        "# import math\n",
        "\n",
        "# # 1. Load saved examples\n",
        "# save_pt_path = \"/content/drive/MyDrive/1_Diamond/CADVLM/omnicad_multimodal_pairs.pt\"\n",
        "# examples = torch.load(save_pt_path)\n",
        "# print(\"Loaded examples:\", len(examples))\n",
        "\n",
        "# # 2. Pick an example that has both images and point clouds, if possible\n",
        "# idx = None\n",
        "# for i, ex in enumerate(examples):\n",
        "#     if ex.get(\"image_paths\") and ex.get(\"pointcloud_paths\"):\n",
        "#         idx = i\n",
        "#         break\n",
        "\n",
        "# if idx is None:\n",
        "#     print(\"No example with both image and pointcloud found; using first example.\")\n",
        "#     idx = 0\n",
        "\n",
        "# ex = examples[idx]\n",
        "# print(\"Chosen example index:\", idx)\n",
        "# print(\"ID:\", ex[\"id\"])\n",
        "# print(\"Category:\", ex[\"category\"], \"Shape ID:\", ex[\"shape_id\"])\n",
        "\n",
        "# # 3. Print the text description\n",
        "# print(\"\\n=== Description ===\")\n",
        "# print(ex[\"description\"])\n",
        "\n",
        "# # 4. Print the CAD JSON (pretty)\n",
        "# print(\"\\n=== CAD JSON ===\")\n",
        "# try:\n",
        "#     cad_obj = json.loads(ex[\"json_text\"])\n",
        "#     print(json.dumps(cad_obj, indent=2))\n",
        "# except Exception as e:\n",
        "#     print(\"[WARN] Could not parse json_text, printing raw string.\")\n",
        "#     print(ex[\"json_text\"])\n",
        "\n",
        "# # 5. Plot images (up to 4)\n",
        "# img_paths = ex.get(\"image_paths\", [])\n",
        "# if img_paths:\n",
        "#     n_imgs = len(img_paths)\n",
        "#     print(f\"\\nShowing all {n_imgs} image(s):\")\n",
        "#     for p in img_paths:\n",
        "#         print(\"  \", p)\n",
        "\n",
        "#     # choose a grid: up to 6 columns, as many rows as needed\n",
        "#     cols = min(6, n_imgs)\n",
        "#     rows = math.ceil(n_imgs / cols)\n",
        "\n",
        "#     fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
        "#     axes = np.array(axes).reshape(-1)  # flatten in case of 2D axes array\n",
        "\n",
        "#     for ax, p in zip(axes[:n_imgs], img_paths):\n",
        "#         img = Image.open(p).convert(\"RGB\")\n",
        "#         ax.imshow(img)\n",
        "#         ax.set_title(os.path.basename(p), fontsize=8)\n",
        "#         ax.axis(\"off\")\n",
        "\n",
        "#     # turn off any extra empty axes if rows*cols > n_imgs\n",
        "#     for ax in axes[n_imgs:]:\n",
        "#         ax.axis(\"off\")\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "# else:\n",
        "#     print(\"\\n[INFO] No image_paths in this example.\")\n",
        "\n",
        "# # 6. Load and plot first point cloud\n",
        "# pc_paths = ex.get(\"pointcloud_paths\", [])\n",
        "# if pc_paths:\n",
        "#     pc_path = pc_paths[0]\n",
        "#     print(\"\\nPoint cloud path:\", pc_path)\n",
        "\n",
        "#     pts = None\n",
        "#     if pc_path.endswith(\".npz\"):\n",
        "#         npz = np.load(pc_path)\n",
        "#         if \"points\" in npz:\n",
        "#             pts = npz[\"points\"]\n",
        "#         else:\n",
        "#             # fall back to first array in the npz\n",
        "#             pts = list(npz.values())[0]\n",
        "#     elif pc_path.endswith(\".npy\"):\n",
        "#         pts = np.load(pc_path)\n",
        "#     else:\n",
        "#         print(\"[WARN] Unsupported point cloud format for plotting:\", pc_path)\n",
        "\n",
        "#     if pts is not None:\n",
        "#         # optional subsampling for speed/visibility\n",
        "#         if pts.shape[0] > 5000:\n",
        "#             sel = np.random.choice(pts.shape[0], 5000, replace=False)\n",
        "#             pts_plot = pts[sel]\n",
        "#         else:\n",
        "#             pts_plot = pts\n",
        "\n",
        "#         fig = plt.figure(figsize=(6, 6))\n",
        "#         ax = fig.add_subplot(111, projection=\"3d\")\n",
        "#         ax.scatter(pts_plot[:, 0], pts_plot[:, 1], pts_plot[:, 2], s=1)\n",
        "#         ax.set_xlabel(\"X\")\n",
        "#         ax.set_ylabel(\"Y\")\n",
        "#         ax.set_zlabel(\"Z\")\n",
        "#         ax.set_title(os.path.basename(pc_path))\n",
        "#         plt.tight_layout()\n",
        "#         plt.show()\n",
        "#     else:\n",
        "#         print(\"[WARN] Could not load point cloud data.\")\n",
        "# else:\n",
        "#     print(\"\\n[INFO] No pointcloud_paths in this example.\")\n"
      ],
      "metadata": {
        "id": "jC0HJqzGQsJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NeuralCarver/Michelangelo.git"
      ],
      "metadata": {
        "id": "l29ddXi5h1jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install omegaconf einops\n",
        "!pip install pytorch-lightning"
      ],
      "metadata": {
        "id": "-KGIggzJiL80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"transformers>=4.45.0\" accelerate bitsandbytes sentencepiece safetensors"
      ],
      "metadata": {
        "id": "HufVjnDppuxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Michelangelo"
      ],
      "metadata": {
        "id": "mslwy4Upa4Rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "# Assuming this import still works and the michelangelo folder is accessible\n",
        "from  michelangelo.models.tsal.sal_perceiver import AlignedShapeLatentPerceiver\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "ENCODER_SD_PATH  = \"/content/drive/MyDrive/1_Diamond/CADVLM/Projects/michelangelo_point_encoder_state_dict.pt\"\n",
        "ENCODER_CFG_PATH = \"/content/drive/MyDrive/1_Diamond/CADVLM/Projects/michelangelo_point_encoder_cfg.yaml\"\n",
        "\n",
        "# 1) Load the tiny shape-module config\n",
        "shape_cfg = OmegaConf.load(ENCODER_CFG_PATH)\n",
        "shape_params = dict(shape_cfg.params)  # OmegaConf -> plain dict\n",
        "\n",
        "# 2) Instantiate the encoder directly (no Lightning, no CLIP)\n",
        "# FIX: Explicitly pass device and dtype required by the constructor\n",
        "michelangelo_encoder = AlignedShapeLatentPerceiver(\n",
        "    **shape_params,\n",
        "    device=device,\n",
        "    dtype=torch.float32  # Using standard float precision\n",
        ")\n",
        "\n",
        "michelangelo_encoder.eval()\n",
        "\n",
        "# 3) Load the saved weights\n",
        "# NOTE: The model is already on 'device' from step 2\n",
        "state_dict = torch.load(ENCODER_SD_PATH, map_location=device)\n",
        "michelangelo_encoder.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "print(\"âœ… Loaded standalone Michelangelo point encoder\")\n",
        "\n",
        "class MichelangeloPointTokens(nn.Module):\n",
        "    \"\"\"\n",
        "    Wraps Michelangelo encoder + PointProjector to produce\n",
        "    [B, 1, d_model] tokens suitable for feeding into LLaVA.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_encoder, projector, freeze_base: bool = True):\n",
        "        super().__init__()\n",
        "        self.base_encoder = base_encoder\n",
        "        self.projector = projector\n",
        "\n",
        "        if freeze_base:\n",
        "            for p in self.base_encoder.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "    def forward(self, points, feats):\n",
        "        \"\"\"\n",
        "        points: [B, N, 3]\n",
        "        feats:  [B, N, F] (per-point features; can be zeros)\n",
        "        returns: pc_tokens [B, 1, d_model]\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            shape_embed, kl_embed, posterior = self.base_encoder.encode(\n",
        "                points,\n",
        "                feats,\n",
        "                sample_posterior=False,\n",
        "            )\n",
        "            # shape_embed: [B, 768]\n",
        "\n",
        "        pc_vec = self.projector(shape_embed)  # [B, d_model]\n",
        "        pc_tokens = pc_vec.unsqueeze(1)       # [B, 1, d_model]\n",
        "        return pc_tokens\n"
      ],
      "metadata": {
        "id": "O7CPwf73jM3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PointProjector(nn.Module):\n",
        "    \"\"\"\n",
        "    Projects a global point-cloud latent (e.g. Michelangelo shape_embed)\n",
        "    into the LLaVA / LLM hidden dimension.\n",
        "\n",
        "    in_dim:  dimensionality of shape_embed (768 for Michelangelo)\n",
        "    out_dim: LLaVA hidden size (llava.config.hidden_size)\n",
        "    hidden_dim: if not None, use a 2-layer MLP; else, a single Linear.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim: int, out_dim: int, hidden_dim: int = None):\n",
        "        super().__init__()\n",
        "        if hidden_dim is None:\n",
        "            # simplest: direct linear projection\n",
        "            self.proj = nn.Linear(in_dim, out_dim)\n",
        "        else:\n",
        "            # slightly richer: MLP with GELU\n",
        "            self.proj = nn.Sequential(\n",
        "                nn.Linear(in_dim, hidden_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(hidden_dim, out_dim),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: [B, in_dim] (e.g., shape_embed)\n",
        "        returns: [B, out_dim]\n",
        "        \"\"\"\n",
        "        return self.proj(x)\n"
      ],
      "metadata": {
        "id": "fyIa1rHTo4oF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "model_id = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\" if device == \"cuda\" else None,\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "print(\"Loaded Qwen2-VL model\")\n"
      ],
      "metadata": {
        "id": "sHqO2Pu98SEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if hasattr(model.config, \"text_config\"):\n",
        "    d_model = model.config.text_config.hidden_size\n",
        "else:\n",
        "    d_model = model.config.hidden_size\n",
        "\n",
        "print(\"Qwen2-VL hidden size:\", d_model)\n"
      ],
      "metadata": {
        "id": "gqCaFyK-snt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_pc = 768\n",
        "\n",
        "point_projector = PointProjector(in_dim=d_pc, out_dim=d_model).to(device)\n",
        "pc_token_encoder = MichelangeloPointTokens(\n",
        "    base_encoder= michelangelo_encoder,\n",
        "    projector=point_projector,\n",
        "    freeze_base=True,   # unfreeze later if you want\n",
        ").to(device)\n",
        "\n",
        "# test\n",
        "B, N = 2, 2048\n",
        "point_feats = shape_params.get(\"point_feats\", 3)\n",
        "dummy_points = torch.randn(B, N, 3, device=device)\n",
        "dummy_feats  = torch.zeros(B, N, point_feats, device=device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    pc_tokens = pc_token_encoder(dummy_points, dummy_feats)\n",
        "\n",
        "print(\"pc_tokens shape:\", pc_tokens.shape)  # should be [2, 1, d_model]\n"
      ],
      "metadata": {
        "id": "a15OKDnzoUJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = processor.tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token  # safety\n",
        "model.config.use_cache = False\n"
      ],
      "metadata": {
        "id": "5n0XhfsA94bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from torch.utils.data import Dataset, random_split\n",
        "\n",
        "jsonl_path = \"/content/drive/MyDrive/1_Diamond/CADVLM/omnicad_multimodal_pairs.jsonl\"\n",
        "\n",
        "class OmniCADMultimodalJSONDataset(Dataset):\n",
        "    def __init__(self, jsonl_path, max_samples=None):\n",
        "        self.examples = []\n",
        "        with open(jsonl_path, \"r\") as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if max_samples is not None and i >= max_samples:\n",
        "                    break\n",
        "                ex = json.loads(line)\n",
        "\n",
        "                desc      = ex.get(\"description\")\n",
        "                json_text = ex.get(\"json_text\")\n",
        "                pc_paths  = ex.get(\"pointcloud_paths\", [])\n",
        "\n",
        "                if desc is None or json_text is None or not pc_paths:\n",
        "                    continue\n",
        "\n",
        "                self.examples.append({\n",
        "                    \"description\": desc,\n",
        "                    \"json_text\":   json_text,\n",
        "                    \"point_paths\": pc_paths,\n",
        "                })\n",
        "\n",
        "        print(f\"Loaded {len(self.examples)} clean examples from\", jsonl_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]\n",
        "\n",
        "full_dataset = OmniCADMultimodalJSONDataset(jsonl_path, max_samples=None)\n",
        "\n",
        "train_ratio = 0.9\n",
        "n_total = len(full_dataset)\n",
        "n_train = int(train_ratio * n_total)\n",
        "n_val   = n_total - n_train\n",
        "\n",
        "train_dataset, val_dataset = random_split(full_dataset, [n_train, n_val])\n",
        "print(\"train_dataset size:\", len(train_dataset))\n",
        "print(\"val_dataset size:  \", len(val_dataset))\n"
      ],
      "metadata": {
        "id": "eWRTGzsZTx-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MichelangeloMultiViewTokens(nn.Module):\n",
        "    \"\"\"\n",
        "    points: [B, V, N, 3]\n",
        "    feats:  [B, V, N, F]\n",
        "    view_mask: [B, V] boolean (True for real views)\n",
        "    returns: pc_tokens [B, 1, d_model]\n",
        "    \"\"\"\n",
        "    def __init__(self, base_encoder, projector, freeze_base=True):\n",
        "        super().__init__()\n",
        "        self.base_encoder = base_encoder\n",
        "        self.projector = projector\n",
        "\n",
        "        if freeze_base:\n",
        "            for p in self.base_encoder.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "    def forward(self, points, feats, view_mask=None):\n",
        "        B, V, N, _ = points.shape\n",
        "        device = points.device\n",
        "\n",
        "        # flatten view dimension into batch\n",
        "        pts_flat   = points.view(B * V, N, 3)\n",
        "        feats_flat = feats.view(B * V, N, feats.shape[-1])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            shape_embed, kl_embed, posterior = self.base_encoder.encode(\n",
        "                pts_flat,\n",
        "                feats_flat,\n",
        "                sample_posterior=False\n",
        "            )\n",
        "            # shape_embed: [B*V, d_pc]\n",
        "\n",
        "        d_pc = shape_embed.shape[-1]\n",
        "        shape_embed = shape_embed.view(B, V, d_pc)   # [B, V, d_pc]\n",
        "\n",
        "        if view_mask is not None:\n",
        "            mask = view_mask.to(shape_embed.device).unsqueeze(-1)   # [B, V, 1] bool\n",
        "            shape_embed = shape_embed.masked_fill(~mask, 0.0)\n",
        "            denom = mask.sum(dim=1).clamp(min=1).float()           # [B, 1, 1]\n",
        "            pooled = shape_embed.sum(dim=1) / denom.squeeze(-1)    # [B, d_pc]\n",
        "        else:\n",
        "            pooled = shape_embed.mean(dim=1)                        # [B, d_pc]\n",
        "\n",
        "        pc_vec    = self.projector(pooled)           # [B, d_model]\n",
        "        pc_tokens = pc_vec.unsqueeze(1)              # [B, 1, d_model]\n",
        "        return pc_tokens\n"
      ],
      "metadata": {
        "id": "O-J0SUO9-Gtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pc_token_encoder = MichelangeloMultiViewTokens(\n",
        "    base_encoder=michelangelo_encoder,\n",
        "    projector=point_projector,\n",
        "    freeze_base=True,\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "yaJu_xso-Lex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class OmniCADTriModalJSONDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Each item:\n",
        "      {\n",
        "        \"description\": <text>,\n",
        "        \"json_text\":   <CAD JSON>,\n",
        "        \"image_paths\": [img0, img1, ...],\n",
        "        \"point_paths\": [pc0, ...]   # in your case usually length 1\n",
        "      }\n",
        "    \"\"\"\n",
        "    def __init__(self, jsonl_path, max_samples=None,\n",
        "                 require_image=True, require_pointcloud=True):\n",
        "        self.examples = []\n",
        "        with open(jsonl_path, \"r\") as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if max_samples is not None and i >= max_samples:\n",
        "                    break\n",
        "                ex = json.loads(line)\n",
        "\n",
        "                desc      = ex.get(\"description\")\n",
        "                json_text = ex.get(\"json_text\")\n",
        "                img_paths = ex.get(\"image_paths\", [])\n",
        "                pc_paths  = ex.get(\"pointcloud_paths\", [])\n",
        "\n",
        "                if desc is None or json_text is None:\n",
        "                    continue\n",
        "                if require_image and not img_paths:\n",
        "                    continue\n",
        "                if require_pointcloud and not pc_paths:\n",
        "                    continue\n",
        "\n",
        "                self.examples.append({\n",
        "                    \"description\": desc,\n",
        "                    \"json_text\":   json_text,\n",
        "                    \"image_paths\": img_paths,\n",
        "                    \"point_paths\": pc_paths,\n",
        "                })\n",
        "\n",
        "        print(f\"Loaded {len(self.examples)} tri-modal examples from {jsonl_path}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]\n"
      ],
      "metadata": {
        "id": "HyTSwwoA__LO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jsonl_path = \"/content/drive/MyDrive/1_Diamond/CADVLM/omnicad_multimodal_pairs.jsonl\"\n",
        "full_dataset = OmniCADTriModalJSONDataset(jsonl_path)\n",
        "\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "train_ratio = 0.9\n",
        "n_total = len(full_dataset)\n",
        "n_train = int(train_ratio * n_total)\n",
        "n_val   = n_total - n_train\n",
        "\n",
        "train_dataset, val_dataset = random_split(full_dataset, [n_train, n_val])\n",
        "print(\"train_dataset size:\", len(train_dataset))\n",
        "print(\"val_dataset size:  \", len(val_dataset))\n"
      ],
      "metadata": {
        "id": "ZFQUNuu8AALq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "N_POINTS    = 2048\n",
        "POINT_FEATS = 3\n",
        "\n",
        "train_prompt_template = (\n",
        "    \"You are a CAD program generator.\\n\"\n",
        "    \"Given a natural language description of a CAD part, \"\n",
        "    \"generate a valid Omni-CAD JSON program that describes the part.\\n\\n\"\n",
        "    \"Description:\\n{description}\\n\\nJSON:\\n{json_text}\\n\"\n",
        ")\n",
        "\n",
        "def load_pointcloud_simple(path, n_points=N_POINTS):\n",
        "    ext = os.path.splitext(path)[1].lower()\n",
        "    if ext == \".npz\":\n",
        "        data = np.load(path)\n",
        "        if \"points\" in data:\n",
        "            pts = data[\"points\"]\n",
        "        elif \"pos\" in data:\n",
        "            pts = data[\"pos\"]\n",
        "        else:\n",
        "            pts = list(data.values())[0]\n",
        "    elif ext == \".npy\":\n",
        "        pts = np.load(path)\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Unsupported pointcloud format: {ext}\")\n",
        "\n",
        "    if pts.shape[1] > 3:\n",
        "        pts = pts[:, :3]\n",
        "\n",
        "    if pts.shape[0] >= n_points:\n",
        "        idx = np.random.choice(pts.shape[0], n_points, replace=False)\n",
        "        pts = pts[idx]\n",
        "    else:\n",
        "        pad = np.zeros((n_points - pts.shape[0], 3), dtype=pts.dtype)\n",
        "        pts = np.concatenate([pts, pad], axis=0)\n",
        "\n",
        "    return pts.astype(np.float32)\n",
        "\n",
        "\n",
        "class CADTriModalCollatorRandomView:\n",
        "    \"\"\"\n",
        "    Tri-modal collator:\n",
        "      - text: description + ground-truth JSON\n",
        "      - image: **ONE random view** per shape\n",
        "      - pointcloud: one pc per shape\n",
        "\n",
        "    This is lighter than multi-view but still tri-modal.\n",
        "    \"\"\"\n",
        "    def __init__(self, processor, n_points=N_POINTS, point_feats=POINT_FEATS,\n",
        "                 max_length=1024):\n",
        "        self.processor   = processor             # Qwen2-VL processor\n",
        "        self.tokenizer   = processor.tokenizer\n",
        "        self.n_points    = n_points\n",
        "        self.point_feats = point_feats\n",
        "        self.max_length  = max_length\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        batch_chat_prompts = []\n",
        "        batch_images       = []   # one PIL image per sample\n",
        "        pc_points_list     = []\n",
        "        pc_feats_list      = []\n",
        "        view_masks_list    = []\n",
        "\n",
        "        for ex in batch:\n",
        "            desc        = ex[\"description\"]\n",
        "            json_text   = ex[\"json_text\"]\n",
        "            image_paths = ex[\"image_paths\"]\n",
        "            point_paths = ex[\"point_paths\"]\n",
        "\n",
        "            # ---- text: description + GT JSON ----\n",
        "            text = train_prompt_template.format(\n",
        "                description=desc,\n",
        "                json_text=json_text,\n",
        "            )\n",
        "\n",
        "            # one image token in the chat, because we sample ONE view\n",
        "            messages = [{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\"},\n",
        "                    {\"type\": \"text\", \"text\": text},\n",
        "                ],\n",
        "            }]\n",
        "\n",
        "            chat_prompt = self.processor.apply_chat_template(\n",
        "                messages,\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=False,\n",
        "            )\n",
        "            batch_chat_prompts.append(chat_prompt)\n",
        "\n",
        "            # ---- ONE random image view ----\n",
        "            if len(image_paths) == 0:\n",
        "                raise ValueError(\"Example has no image_paths\")\n",
        "\n",
        "            img_path = random.choice(image_paths)\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "            batch_images.append(img)\n",
        "\n",
        "            # ---- ONE pointcloud ----\n",
        "            if len(point_paths) == 0:\n",
        "                raise ValueError(\"Example has no point_paths\")\n",
        "\n",
        "            pc_path = point_paths[0]\n",
        "            pts = load_pointcloud_simple(pc_path, n_points=self.n_points)  # [N,3]\n",
        "            pts = torch.from_numpy(pts)  # [N,3]\n",
        "\n",
        "            pts = pts.unsqueeze(0)  # [V=1,N,3]\n",
        "            feats = torch.zeros(1, self.n_points, self.point_feats, dtype=torch.float32)\n",
        "            view_mask = torch.ones(1, dtype=torch.bool)  # one valid pc view\n",
        "\n",
        "            pc_points_list.append(pts)\n",
        "            pc_feats_list.append(feats)\n",
        "            view_masks_list.append(view_mask)\n",
        "\n",
        "        # ---- Qwen2-VL processor: text + single image per sample ----\n",
        "        proc_inputs = self.processor(\n",
        "            text=batch_chat_prompts,\n",
        "            images=batch_images,          # [B] list of PIL images\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "        )\n",
        "\n",
        "        input_ids      = proc_inputs[\"input_ids\"]\n",
        "        attention_mask = proc_inputs[\"attention_mask\"]\n",
        "        labels         = input_ids.clone()   # predict every token for now\n",
        "\n",
        "        # stack pc tensors -> [B,1,N,3], [B,1,N,F], [B,1]\n",
        "        points    = torch.stack(pc_points_list,  dim=0)\n",
        "        feats     = torch.stack(pc_feats_list,   dim=0)\n",
        "        view_mask = torch.stack(view_masks_list, dim=0)\n",
        "\n",
        "        batch_out = {\n",
        "            \"input_ids\":      input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\":         labels,\n",
        "            \"points\":         points,\n",
        "            \"feats\":          feats,\n",
        "            \"view_mask\":      view_mask,\n",
        "        }\n",
        "\n",
        "        # add vision kwargs (pixel_values, image_grid_thw, etc.)\n",
        "        for k, v in proc_inputs.items():\n",
        "            if k not in [\"input_ids\", \"attention_mask\"]:\n",
        "                batch_out[k] = v\n",
        "\n",
        "        return batch_out\n"
      ],
      "metadata": {
        "id": "wAme0E7GAB3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CADQwen2VL(nn.Module):\n",
        "    def __init__(self, base_model, pc_token_encoder, tokenizer):\n",
        "        super().__init__()\n",
        "        self.base_model       = base_model          # Qwen2-VL with LoRA\n",
        "        self.pc_token_encoder = pc_token_encoder    # Michelangelo+projector\n",
        "        self.tokenizer        = tokenizer\n",
        "\n",
        "        special_tokens = [\"<PCD_START>\", \"<PCD_END>\"]\n",
        "        added = self.tokenizer.add_special_tokens(\n",
        "            {\"additional_special_tokens\": special_tokens}\n",
        "        )\n",
        "        if added > 0:\n",
        "            self.base_model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "        self.PCD_START_ID = self.tokenizer.convert_tokens_to_ids(\"<PCD_START>\")\n",
        "        self.PCD_END_ID   = self.tokenizer.convert_tokens_to_ids(\"<PCD_END>\")\n",
        "\n",
        "        self.embed_tokens = self.base_model.get_input_embeddings()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask,\n",
        "        labels=None,\n",
        "        points=None,\n",
        "        feats=None,\n",
        "        view_mask=None,\n",
        "        pixel_values=None,\n",
        "        **vision_kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        During training, Trainer will pass:\n",
        "          - input_ids, attention_mask, labels\n",
        "          - points, feats, view_mask (from collator)\n",
        "          - pixel_values (+ image_grid_thw, etc. inside **vision_kwargs)\n",
        "        \"\"\"\n",
        "        device = input_ids.device\n",
        "        B, L   = input_ids.shape\n",
        "\n",
        "        # ---- 1) point cloud -> pc token ----\n",
        "        if points is not None:\n",
        "            pc_tokens = self.pc_token_encoder(\n",
        "                points.to(device),\n",
        "                feats.to(device),\n",
        "                view_mask=view_mask.to(device),\n",
        "            )  # [B,1,d]\n",
        "            d_emb = self.embed_tokens.weight.dtype\n",
        "            pc_tokens = pc_tokens.to(device=device, dtype=d_emb)\n",
        "        else:\n",
        "            pc_tokens = None\n",
        "\n",
        "        # ---- 2) text embeddings ----\n",
        "        text_embeds = self.embed_tokens(input_ids)  # [B,L,d]\n",
        "\n",
        "        # ---- 3) prepend <PCD_START> [pc_token] <PCD_END> ----\n",
        "        if pc_tokens is not None:\n",
        "            pcd_start_ids = torch.full(\n",
        "                (B, 1),\n",
        "                self.PCD_START_ID,\n",
        "                device=device,\n",
        "                dtype=torch.long,\n",
        "            )\n",
        "            pcd_end_ids = torch.full(\n",
        "                (B, 1),\n",
        "                self.PCD_END_ID,\n",
        "                device=device,\n",
        "                dtype=torch.long,\n",
        "            )\n",
        "\n",
        "            pcd_start_emb = self.embed_tokens(pcd_start_ids)  # [B,1,d]\n",
        "            pcd_end_emb   = self.embed_tokens(pcd_end_ids)    # [B,1,d]\n",
        "\n",
        "            prefix_embeds = torch.cat([pcd_start_emb, pc_tokens, pcd_end_emb], dim=1)  # [B,3,d]\n",
        "            full_embeds   = torch.cat([prefix_embeds, text_embeds], dim=1)             # [B,3+L,d]\n",
        "\n",
        "            prefix_mask    = torch.ones(B, prefix_embeds.size(1),\n",
        "                                        device=device, dtype=attention_mask.dtype)\n",
        "            full_attn_mask = torch.cat([prefix_mask, attention_mask], dim=1)           # [B,3+L]\n",
        "        else:\n",
        "            full_embeds   = text_embeds\n",
        "            full_attn_mask = attention_mask\n",
        "\n",
        "        # ---- 4) extend labels to match new length ----\n",
        "        if labels is not None:\n",
        "            prefix_len = full_embeds.size(1) - L\n",
        "            extended_labels = torch.full(\n",
        "                (B, full_embeds.size(1)),\n",
        "                -100,\n",
        "                device=device,\n",
        "                dtype=labels.dtype,\n",
        "            )\n",
        "            extended_labels[:, prefix_len:] = labels\n",
        "        else:\n",
        "            extended_labels = None\n",
        "\n",
        "        # ---- 5) call Qwen2-VL base model ----\n",
        "        outputs = self.base_model(\n",
        "            inputs_embeds=full_embeds,\n",
        "            attention_mask=full_attn_mask,\n",
        "            labels=extended_labels,\n",
        "            pixel_values=pixel_values,\n",
        "            **vision_kwargs,  # e.g. image_grid_thw\n",
        "        )\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "Pd92OyxcAGqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# 1) Load Qwen2-VL and processor (skip if already loaded)\n",
        "qwen_model_id = \"Qwen/Qwen2-VL-2B-Instruct\"   # or whatever you used\n",
        "processor = Qwen2VLProcessor.from_pretrained(qwen_model_id)\n",
        "\n",
        "base_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    qwen_model_id,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\" if device == \"cuda\" else None,\n",
        ")\n",
        "base_model.eval()\n",
        "\n",
        "# 2) LoRA config for Qwen2-VL\n",
        "target_modules = [\n",
        "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",   # attention\n",
        "    \"gate_proj\", \"up_proj\", \"down_proj\",      # MLP\n",
        "]\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=target_modules,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "qwen_lora = get_peft_model(base_model, lora_config)\n",
        "qwen_lora.print_trainable_parameters()\n",
        "\n",
        "# If you don't have pc_token_encoder yet, (re)create it:\n",
        "pc_token_encoder = MichelangeloMultiViewTokens(\n",
        "    base_encoder=michelangelo_encoder,\n",
        "    projector=point_projector,\n",
        "    freeze_base=True,   # or False if you want to fine-tune Michelangelo too\n",
        ").to(device)\n",
        "\n",
        "# 4) Build CADQwen2VL wrapper\n",
        "cad_model = CADQwen2VL(\n",
        "    base_model=qwen_lora,\n",
        "    pc_token_encoder=pc_token_encoder,\n",
        "    tokenizer=processor.tokenizer,\n",
        ").to(device)\n",
        "\n",
        "# 5) Freeze everything, then unfreeze LoRA + point projector\n",
        "for p in cad_model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "for name, p in cad_model.named_parameters():\n",
        "    if \"lora_\" in name:  # LoRA modules\n",
        "        p.requires_grad = True\n",
        "    if \"pc_token_encoder.projector\" in name:  # point projector\n",
        "        p.requires_grad = True\n",
        "\n",
        "trainable = sum(p.numel() for p in cad_model.parameters() if p.requires_grad)\n",
        "total     = sum(p.numel() for p in cad_model.parameters())\n",
        "print(f\"Trainable params: {trainable/1e6:.2f}M / {total/1e6:.2f}M\")\n"
      ],
      "metadata": {
        "id": "_d8VGBc6Ad3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "E0UjQBx-IVOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "collator = CADTriModalCollatorRandomView(processor)\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/1_Diamond/CADVLM/qwen2vl_triple_lora\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=1e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=50,\n",
        "    save_steps=50,\n",
        "    save_total_limit=2,\n",
        "    report_to=[],\n",
        "    remove_unused_columns=False,   # important: don't drop our custom fields\n",
        "    save_safetensors=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=cad_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "OGHv0GG8AKSQ",
        "outputId": "68864735-d983-4035-daa2-a6284ceb4f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='5926' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  48/5926 04:33 < 9:41:53, 0.17 it/s, Epoch 0.01/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa\n",
        "from PIL import Image\n",
        "\n",
        "INFER_PROMPT_TEMPLATE = (\n",
        "    \"You are a CAD program generator.\\n\"\n",
        "    \"Given a natural language description of a CAD part, \"\n",
        "    \"generate a valid Omni-CAD JSON program that describes the part.\\n\\n\"\n",
        "    \"Description:\\n{description}\\n\\nJSON:\\n\"\n",
        ")\n",
        "\n",
        "def extract_json_block(text: str) -> str:\n",
        "    start = text.find(\"{\")\n",
        "    end = text.rfind(\"}\")\n",
        "    if start == -1 or end == -1 or end <= start:\n",
        "        return text.strip()\n",
        "    return text[start : end + 1].strip()\n",
        "\n",
        "\n",
        "def run_val_inference_single_view(\n",
        "    idx=None,\n",
        "    max_new_tokens=512,\n",
        "    n_points=2048,\n",
        "    point_feats=3,\n",
        "):\n",
        "    \"\"\"\n",
        "    One-sample inference:\n",
        "      text + ONE random image view + point cloud.\n",
        "    Visualizes inputs and prints predicted JSON.\n",
        "    \"\"\"\n",
        "\n",
        "    cad_model.eval()\n",
        "\n",
        "    # ----- 1. pick a val example -----\n",
        "    if idx is None:\n",
        "        idx = random.randint(0, len(val_dataset) - 1)\n",
        "\n",
        "    ex = val_dataset[idx]\n",
        "    desc        = ex[\"description\"]\n",
        "    json_text   = ex[\"json_text\"]\n",
        "    image_paths = ex[\"image_paths\"]\n",
        "    point_paths = ex[\"point_paths\"]\n",
        "\n",
        "    print(f\"\\n=== Tri-modal inference (single view) on val idx = {idx} ===\\n\")\n",
        "    print(\"Description:\\n\", desc)\n",
        "    print(\"\\nGround-truth JSON snippet:\\n\", json_text[:600], \"...\\n\")\n",
        "\n",
        "    # ----- 2. visualize text -----\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Input description + GT JSON snippet\")\n",
        "    snippet = json_text[:400]\n",
        "    txt_block = f\"Description:\\n{desc}\\n\\nGT JSON (truncated):\\n{snippet}\"\n",
        "    plt.text(\n",
        "        0.01,\n",
        "        0.99,\n",
        "        txt_block,\n",
        "        va=\"top\",\n",
        "        ha=\"left\",\n",
        "        fontsize=8,\n",
        "        family=\"monospace\",\n",
        "        wrap=True,\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "    # ----- 3. choose ONE random image view -----\n",
        "    if len(image_paths) == 0:\n",
        "        print(\"No images for this example; aborting.\")\n",
        "        return\n",
        "\n",
        "    img_path = random.choice(image_paths)\n",
        "    print(\"[IMG] using single view:\", img_path)\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Random chosen view\")\n",
        "    plt.show()\n",
        "\n",
        "    # ----- 4. load + plot point cloud -----\n",
        "    if len(point_paths) == 0:\n",
        "        print(\"No pointcloud for this example; aborting.\")\n",
        "        return\n",
        "\n",
        "    pc_path = point_paths[0]\n",
        "    print(\"[PC] using pointcloud:\", pc_path)\n",
        "\n",
        "    pts = load_pointcloud_simple(pc_path, n_points=n_points)  # [N,3]\n",
        "    pts_plot = pts\n",
        "    if pts_plot.shape[0] > 5000:\n",
        "        idx_vis = np.random.choice(pts_plot.shape[0], 5000, replace=False)\n",
        "        pts_plot = pts_plot[idx_vis]\n",
        "\n",
        "    fig = plt.figure(figsize=(5, 5))\n",
        "    ax = fig.add_subplot(111, projection=\"3d\")\n",
        "    ax.scatter(\n",
        "        pts_plot[:, 0],\n",
        "        pts_plot[:, 1],\n",
        "        pts_plot[:, 2],\n",
        "        s=1,\n",
        "    )\n",
        "    ax.set_title(\"Input point cloud\")\n",
        "    plt.show()\n",
        "\n",
        "    # ----- 5. build Qwen2-VL inputs: text + 1 image -----\n",
        "    prompt = INFER_PROMPT_TEMPLATE.format(description=desc)\n",
        "\n",
        "    messages = [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": prompt},\n",
        "        ],\n",
        "    }]\n",
        "\n",
        "    chat_prompt = processor.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "\n",
        "    proc_inputs = processor(\n",
        "        text=[chat_prompt],\n",
        "        images=[image],   # single PIL image in a list\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    input_ids      = proc_inputs[\"input_ids\"]\n",
        "    attention_mask = proc_inputs[\"attention_mask\"]\n",
        "    vision_kwargs = {\n",
        "        k: v for k, v in proc_inputs.items()\n",
        "        if k not in [\"input_ids\", \"attention_mask\"]\n",
        "    }\n",
        "\n",
        "    # ----- 6. prepare pc token -----\n",
        "    pts_torch = torch.from_numpy(pts).to(device)      # [N,3]\n",
        "    pts_torch = pts_torch.unsqueeze(0).unsqueeze(0)   # [1,1,N,3]\n",
        "    feats     = torch.zeros(1, 1, n_points, point_feats,\n",
        "                            dtype=torch.float32, device=device)\n",
        "    view_mask = torch.ones(1, 1, dtype=torch.bool, device=device)\n",
        "\n",
        "    embed_tokens = cad_model.embed_tokens\n",
        "    B, L = input_ids.shape\n",
        "\n",
        "    text_embeds = embed_tokens(input_ids)  # [1,L,d]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pc_tokens = cad_model.pc_token_encoder(\n",
        "            pts_torch,\n",
        "            feats,\n",
        "            view_mask=view_mask,\n",
        "        )  # [1,1,d]\n",
        "        pc_tokens = pc_tokens.to(\n",
        "            device=device,\n",
        "            dtype=embed_tokens.weight.dtype,\n",
        "        )\n",
        "\n",
        "    pcd_start_ids = torch.full(\n",
        "        (B, 1),\n",
        "        cad_model.PCD_START_ID,\n",
        "        device=device,\n",
        "        dtype=torch.long,\n",
        "    )\n",
        "    pcd_end_ids = torch.full(\n",
        "        (B, 1),\n",
        "        cad_model.PCD_END_ID,\n",
        "        device=device,\n",
        "        dtype=torch.long,\n",
        "    )\n",
        "\n",
        "    pcd_start_emb = embed_tokens(pcd_start_ids)\n",
        "    pcd_end_emb   = embed_tokens(pcd_end_ids)\n",
        "\n",
        "    prefix_embeds = torch.cat(\n",
        "        [pcd_start_emb, pc_tokens, pcd_end_emb], dim=1\n",
        "    )  # [1,3,d]\n",
        "    full_embeds = torch.cat(\n",
        "        [prefix_embeds, text_embeds], dim=1\n",
        "    )  # [1,3+L,d]\n",
        "\n",
        "    prefix_mask    = torch.ones(B, prefix_embeds.size(1),\n",
        "                                device=device,\n",
        "                                dtype=attention_mask.dtype)\n",
        "    full_attn_mask = torch.cat([prefix_mask, attention_mask], dim=1)  # [1,3+L]\n",
        "\n",
        "    # ----- 7. generate JSON -----\n",
        "    with torch.no_grad():\n",
        "        out_ids = cad_model.base_model.generate(\n",
        "            inputs_embeds=full_embeds,\n",
        "            attention_mask=full_attn_mask,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            **vision_kwargs,\n",
        "        )\n",
        "\n",
        "    out_text = processor.tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
        "    pred_json = extract_json_block(out_text)\n",
        "\n",
        "    # ----- 8. show predicted JSON -----\n",
        "    print(\"\\nPredicted JSON (truncated):\\n\")\n",
        "    print(pred_json[:2000], \"...\\n\")\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Predicted JSON (first ~2000 chars)\")\n",
        "    plt.text(\n",
        "        0.01,\n",
        "        0.99,\n",
        "        pred_json[:2000],\n",
        "        va=\"top\",\n",
        "        ha=\"left\",\n",
        "        fontsize=8,\n",
        "        family=\"monospace\",\n",
        "        wrap=True,\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        \"idx\": idx,\n",
        "        \"description\": desc,\n",
        "        \"gt_json\": json_text,\n",
        "        \"pred_json\": pred_json,\n",
        "        \"image_path\": img_path,\n",
        "        \"point_path\": pc_path,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "ZWoaC1BdEKMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = run_val_inference_single_view(idx=2006)"
      ],
      "metadata": {
        "id": "tJ7_MhPtEx6m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}