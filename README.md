# CAD-MLLM Reproduction

This is a reproduction of the paper "CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM" ([arXiv:2411.04954](https://arxiv.org/abs/2411.04954)).

## Overview

This repository provides a complete pipeline for processing and rendering CAD models from the Omni-CAD dataset. The pipeline includes:

1. **CAD Processing**: Convert JSON CAD sequences to STEP mesh files
2. **CAD Rendering**: Render STEP mesh files to multi-viewpoint images
3. **CAD-MLLM Model**: Multimodal CAD generation from text and images

## Repository Structure

```
CMU16825_Final_project/
├── pipeline/              # Main processing pipeline
│   ├── process_cad.py    # Convert JSON -> STEP
│   └── render_cad.py     # Convert STEP -> Images
├── cad_mllm/             # CAD-MLLM model implementation
├── scripts/              # Additional utilities
├── data/                 # Omni-CAD dataset
│   ├── Omni-CAD/        # Full dataset
│   └── Omni-CAD-subset/ # Subset for development
└── 3rd_party/           # Third-party dependencies
    └── DeepCAD/         # DeepCAD processing utilities
```

## Setup

### Prerequisites

- Python >= 3.10
- Conda (recommended for managing environments)
- `uv` package manager (optional)

### Installation

#### 1. Create Conda Environment for CAD Processing

```bash
# Create environment with OpenCASCADE
conda create -n DeepCAD python=3.10
conda activate DeepCAD
conda install -c conda-forge pythonocc-core=7.8.1

# Install DeepCAD dependencies
cd 3rd_party/DeepCAD
pip install -r requirements.txt
cd ../..
```

#### 2. Install Project Dependencies

```bash
# Using uv (recommended)
uv venv
source .venv/bin/activate  # On Unix/macOS
# or
.venv\Scripts\activate  # On Windows
uv pip install -e .

# Or using pip
pip install -e .
```

## Pipeline: Processing and Rendering CAD Models

### Step 1: Process CAD (JSON → STEP)

Convert JSON CAD sequences from Omni-CAD to STEP mesh files.

```bash
# Activate DeepCAD environment
conda activate DeepCAD

# Run processing
python pipeline/process_cad.py
```

**What it does:**
- Reads all JSON files from `data/Omni-CAD/json/`
- Converts them to STEP mesh files using DeepCAD
- Saves output to `data/Omni-CAD/step/`
- Estimated time: 4-6 hours for full dataset
- Storage needed: ~29 GB

**Output:**
- STEP mesh files in `data/Omni-CAD/step/` (mirroring the JSON directory structure)
- Log file in `logs/process_cad_*.log`

### Step 2: Render CAD (STEP → Images)

Render STEP mesh files to multi-viewpoint images.

```bash
# Activate DeepCAD environment
conda activate DeepCAD

# Render full dataset
python pipeline/render_cad.py \
    --src data/Omni-CAD/step \
    --output data/Omni-CAD/render

# Or render a subset
python pipeline/render_cad.py \
    --src data/Omni-CAD-subset/step \
    --output data/Omni-CAD-subset/render \
    --num_views 8
```

**Options:**
- `--src`: Source directory with STEP files (default: `data/Omni-CAD/step`)
- `--output`: Output directory for rendered images (default: `data/Omni-CAD/render`)
- `--num_views`: Number of viewpoints per model (default: 8)
- `--idx`: Start from index for resuming (default: 0)
- `--num`: Number of files to process, -1 for all (default: -1)

**What it does:**
- Reads STEP files from source directory
- Renders each model from 8 different viewpoints
- Saves PNG images with white background and Phong shading
- Preserves directory structure from source

**Output:**
- Rendered images in `data/Omni-CAD/render/` (same structure as STEP files)
- Each model: 8 images named `{model_id}_{000-007}.png`
- Log file in `logs/render_cad_*.log`

## Dataset Information

### Omni-CAD Dataset Structure

```
data/Omni-CAD/
├── json/          # CAD sequences in JSON format (~605K files, ~14 GB)
├── step/          # STEP mesh files (generated by process_cad.py, ~29 GB)
├── render/        # Rendered images (generated by render_cad.py)
├── txt/           # Text captions (~100 files, ~165 MB)
└── ...
```

### Creating Subsets

Use `analyze_omni_cad.py` to analyze and create subsets:

```bash
# Analyze dataset
python analyze_omni_cad.py --analyze

# Create 10% subset
python analyze_omni_cad.py --subset 10

# Create subset by sampling batches
python analyze_omni_cad.py --subset-batches 10
```

## CAD-MLLM Model Usage

### Text-to-CAD Generation

```bash
python scripts/inference.py \
    --prompt "Generate a CAD model of a simple cube." \
    --device cuda \
    --image_path "data/Omni-CAD/img/cube.jpeg" \
    --dtype bfloat16
```

See `scripts/inference.py` and `cad_mllm/config.py` for more details.

## Testing

### Test Single File Rendering

```bash
conda activate DeepCAD
python test_render_single.py
```

This renders a single test STEP file to verify your setup.

## Utilities

### Check Dataset Size

```bash
python check_dataset_size.py
```

Shows sizes of all dataset directories and estimates final sizes.

## Troubleshooting

### Common Issues

1. **OpenCASCADE Installation Issues**
   ```bash
   # Use conda-forge channel
   conda install -c conda-forge pythonocc-core=7.8.1
   ```

2. **Memory Issues During Processing**
   - Process in smaller batches using `--num` option
   - Use `analyze_omni_cad.py` to create a subset first

3. **Rendering Fails**
   - Ensure `PYOPENGL_PLATFORM=egl` for headless rendering
   - Check that STEP files are valid (test with `test_render_single.py`)

## Project Structure Details

### Pipeline Scripts
- `pipeline/process_cad.py`: Main script for JSON → STEP conversion
- `pipeline/render_cad.py`: Main script for STEP → Images rendering

### Analysis Tools
- `analyze_omni_cad.py`: Dataset analysis and subset creation
- `check_dataset_size.py`: Storage size estimation

### Model Implementation
- `cad_mllm/`: CAD-MLLM model code
  - `model.py`: Main model architecture
  - `encoders/`: Modality encoders (text, image, pointcloud)
  - `projectors/`: Projection layers

## Citation

If you use this code, please cite the original paper:

```bibtex
@misc{xu2024CADMLLM,
    title={CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM},
    author={Jingwei Xu and Chenyu Wang and Zibo Zhao and Wen Liu and Yi Ma and Shenghua Gao},
    year={2024},
    eprint={2411.04954},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
```

## License

This is a research reproduction for educational purposes.
